{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80DlzNJt0M5r"
      },
      "source": [
        "# RAG Fusion\n",
        "\n",
        "Re-implemented from [this GitHub repo](https://github.com/Raudaschl/rag-fusion), all credit to the original author."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opAjXPLKzzQL"
      },
      "source": [
        "## Setup\n",
        "\n",
        "For this example, we will use Pinecone and some fake data. To configure Pinecone, set the following environment variable:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- `PINECONE_API_KEY`: Your Pinecone API key\n",
        "\n",
        "- `OPENAI_API_KEY`: Your OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJ_d8yfv9ig"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "index_name = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhaU7_tq0ZS8"
      },
      "source": [
        "#### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f0dEqFufw4PS",
        "outputId": "30cc65ff-c855-4af1-f569-8d5e8e6ab3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.2.2-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.26.0 (from langchain_openai)\n",
            "  Downloading openai-1.30.4-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (1.25.2)\n",
            "Collecting pinecone-client<4.0.0,>=3.2.2 (from langchain_pinecone)\n",
            "  Downloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_core)\n",
            "  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<24.0,>=23.2 (from langchain_core)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.3.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_core)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2024.2.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pinecone-client, packaging, orjson, jsonpointer, h11, tiktoken, jsonpatch, httpcore, langsmith, httpx, openai, langchain_core, langchain-text-splitters, langchain_pinecone, langchain_openai, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-text-splitters-0.2.0 langchain_core-0.2.2 langchain_openai-0.1.8 langchain_pinecone-0.1.1 langsmith-0.1.63 openai-1.30.4 orjson-3.10.3 packaging-23.2 pinecone-client-3.2.2 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain_openai langchain_pinecone langchain_core langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2r_Fo5HxQiG"
      },
      "source": [
        "#### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvGIa-Bnw3zj"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa-Wti_3xMxx"
      },
      "source": [
        "#### Load Fake Data\n",
        "\n",
        "Here we define a set of fake documents related to climate change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djfCk_qpxMMP"
      },
      "outputs": [],
      "source": [
        "all_documents = {\n",
        "    \"doc1\": \"Climate change and economic impact.\",\n",
        "    \"doc2\": \"Public health concerns due to climate change.\",\n",
        "    \"doc3\": \"Climate change: A social perspective.\",\n",
        "    \"doc4\": \"Technological solutions to climate change.\",\n",
        "    \"doc5\": \"Policy changes needed to combat climate change.\",\n",
        "    \"doc6\": \"Climate change and its impact on biodiversity.\",\n",
        "    \"doc7\": \"Climate change: The science and models.\",\n",
        "    \"doc8\": \"Global warming: A subset of climate change.\",\n",
        "    \"doc9\": \"How climate change affects daily weather.\",\n",
        "    \"doc10\": \"The history of climate change activism.\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_cqV8faxZca"
      },
      "source": [
        "#### Create Vector Store\n",
        "\n",
        "We will use Pinecone to create a vector store from these documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFK8ViaOxT5Q"
      },
      "outputs": [],
      "source": [
        "vectorstore = PineconeVectorStore.from_texts(\n",
        "    list(all_documents.values()), OpenAIEmbeddings(), index_name=index_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjU_T7V-043M"
      },
      "source": [
        "## Define the Query Generator\n",
        "\n",
        "We will now define a chain to do the query generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBmd8bte1KXL"
      },
      "source": [
        "#### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz77xXjixq3t"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WlOcFXBWxzJN",
        "outputId": "cd5e93c2-3ef8-4474-f1c0-5a94cacabf6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.17-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.17 types-requests-2.32.0.20240523\n"
          ]
        }
      ],
      "source": [
        "!pip install langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZpUlRq1PLK"
      },
      "source": [
        "#### Load Prompt From LangChainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdA47rbexuLf"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"langchain-ai/rag-fusion-query-generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYOwGJmJx6Iw"
      },
      "outputs": [],
      "source": [
        "# prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
        "#     (\"user\", \"Generate multiple search queries related to: {original_query}\"),\n",
        "#     (\"user\", \"OUTPUT (4 queries):\")\n",
        "# ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9IZQDMQ1hFp"
      },
      "source": [
        "#### Define Query Generation Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0EDAifOx8w7"
      },
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt | ChatOpenAI(temperature=0) | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi7-FQji1ndQ"
      },
      "source": [
        "## Define the Full Chain\n",
        "\n",
        "We can now put it all together and define the full chain. This chain:\n",
        "\n",
        "1. Generates a bunch of queries\n",
        "2. Looks up each query in the retriever\n",
        "3. Joins all the results together using reciprocal rank fusion\n",
        "\n",
        "Note that it does NOT do a final generation step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiZY3qbB1xh1"
      },
      "source": [
        "#### Original Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl_fDxEoyAT9"
      },
      "outputs": [],
      "source": [
        "original_query = \"impact of climate change\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa8ebg9r1wkG"
      },
      "source": [
        "#### Create Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqd-prFvyDLR"
      },
      "outputs": [],
      "source": [
        "vectorstore = PineconeVectorStore.from_existing_index(index_name, OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBiQvKTq16JO"
      },
      "source": [
        "#### Define Reciprocal Rank Fusion Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sqkoL6ayH6I"
      },
      "outputs": [],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # Assumes the docs are returned in sorted order of relevance\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "    return reranked_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pFpmIHj19-Y"
      },
      "source": [
        "#### Define Full Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj-2Myd7yLaJ"
      },
      "outputs": [],
      "source": [
        "chain = generate_queries | retriever.map() | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPiYkR9K2BGx"
      },
      "source": [
        "#### Invoke Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M3Bk79ZyQ7E",
        "outputId": "e78b3908-b27a-418b-b003-f2cb185792bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(Document(page_content='Climate change and its impact on biodiversity.'),\n",
              "  0.06506215742069787),\n",
              " (Document(page_content='Climate change and economic impact.'),\n",
              "  0.06506215742069787),\n",
              " (Document(page_content='Technological solutions to climate change.'),\n",
              "  0.06506215742069787),\n",
              " (Document(page_content='Climate change: A social perspective.'),\n",
              "  0.06506215742069787)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"original_query\": original_query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoVqsBgmDxMz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
